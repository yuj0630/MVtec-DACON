{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO5ekBfpz8im04Sxs53oncn"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","source":["!pip install timm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PEP_EPZPMuBw","executionInfo":{"status":"ok","timestamp":1678077292821,"user_tz":-540,"elapsed":11038,"user":{"displayName":"정영운","userId":"08661002276326861946"}},"outputId":"37fcfb24-4802-405c-e27e-f885e65955a1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm\n","  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub\n","  Downloading huggingface_hub-0.12.1-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm) (0.14.1+cu116)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from timm) (6.0)\n","Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.8/dist-packages (from timm) (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (23.0)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (4.64.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (2.25.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (3.9.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (8.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (1.22.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (1.26.14)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n","Installing collected packages: huggingface-hub, timm\n","Successfully installed huggingface-hub-0.12.1 timm-0.6.12\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wO1-k2wnMfbb"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import torch\n","import os\n","from tqdm import tqdm\n","from glob import glob\n","import cv2\n","import numpy as np\n","import pandas as pd\n","import PIL \n","from torch import nn\n","from torch import optim\n","import torch.nn.functional as F\n","from torchvision import datasets, transforms\n","import torchvision as tv\n","from PIL import Image"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount._DEBUG = True\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"id":"y_Gn1OpBMwmv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.chdir('./drive/MyDrive/이상치/')"],"metadata":{"id":"e4yMbAfuM0Be"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cpu'\n","if torch.cuda.is_available():\n","    device = 'cuda'\n","    \n","device"],"metadata":{"id":"z15pf_tzM2Bm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_imgs = [img_load(m) for m in tqdm(train_png)]\n","test_imgs = [img_load(n) for n in tqdm(test_png)]"],"metadata":{"id":"LL5GEA9WM2uG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 데이터 증강(data argumentation)"],"metadata":{"id":"YpyrMvg1RExr"}},{"cell_type":"code","source":["# 하는 방법\n","import albumentations\n","import albumentations.pytorch\n","\n","aug = albumentations.Compose([\n","      albumentations.Resize(224, 224),\n","      albumentations.HorizontalFlip(), #수평 뒤집기기 \n","      albumentations.VerticalFlip(), #수직 뒤집기\n","      albumentations.OneOf([\n","                          albumentations.Rotate(), # 돌리기기\n","                          albumentations.ShiftScaleRotate()\n"," \n","      ], p=1),\n","      albumentations.augmentations.transforms.Normalize(mean=(0.5,), std=(0.5,), p=1.0), #정규화화\n","      albumentations.pytorch.transforms.ToTensorV2(p=1.0)\n","      ])\n","aug2 = albumentations.Compose([\n","      albumentations.Resize(224, 224),\n","      albumentations.Rotate(), # 돌리기기\n","      albumentations.augmentations.transforms.Normalize(mean=(0.5,), std=(0.5,), p=1.0),\n","      albumentations.pytorch.transforms.ToTensorV2(p=1.0)\n","      ])"],"metadata":{"id":"M5R8cUNIM6du"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#  학습(다른 예시)"],"metadata":{"id":"U15DYcd0ROmh"}},{"cell_type":"code","source":["# 시도 횟수\n","n_epochs = 100\n","valid_loss_min = np.inf # 100보다 작을 시 멈춤\n","\n","# keep track of training and validation loss\n","train_loss = torch.zeros(n_epochs)\n","valid_loss = torch.zeros(n_epochs)\n","\n","train_F1 = torch.zeros(n_epochs)\n","valid_F1 = torch.zeros(n_epochs)\n","model.to(device)\n","\n","for e in range(0, n_epochs):\n","\n","   \n","    ###################\n","    # 모델 학습       #\n","    ###################\n","    model.train()\n","    for data, labels in tqdm(train_dataloader):\n","        # move tensors to GPU if CUDA is available\n","        data, labels = data.to(device), labels.to(device)\n","        # 순전파 :compute predicted outputs by passing inputs to the model\n","        logits = model(data)\n","        # 배치의 손실율 계산\n","        loss = criterion(logits, labels)\n","\n","        optimizer.zero_grad()\n","        # 역전파 : compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # 싱글 옵티미제이션(adam)\n","        optimizer.step()\n","        # 학습 손실율 업데이트\n","        train_loss[e] += loss.item()\n","        # 학습 스코어 계산\n","        logits=logits.argmax(1).detach().cpu().numpy().tolist()\n","        labels=labels.detach().cpu().numpy().tolist()\n","\n","        train_F1[e] += score_function(labels,logits)\n","\n","    train_loss[e] /= len(train_dataloader)\n","    train_F1[e] /= len(train_dataloader)\n","        \n","        \n","    ######################    \n","    # 검증 모델          #\n","    ######################\n","    with torch.no_grad(): \n","        model.eval()\n","        for data, labels in tqdm(valid_dataloader):\n","            # move tensors to GPU if CUDA is available\n","            data, labels = data.to(device), labels.to(device)\n","            # 순전파: compute predicted outputs by passing inputs to the model\n","            logits = model(data)\n","            # 배치 손실율 계산\n","            loss = criterion(logits, labels)\n","            # 평균 검증 손실율 \n","            valid_loss[e] += loss.item()\n","            # update training score\n","            logits=logits.argmax(1).detach().cpu().numpy().tolist()\n","            labels=labels.detach().cpu().numpy().tolist()\n","            valid_F1[e] += score_function(labels,logits)\n","            \n","    \n","    # 평균 손실율 계산산\n","    valid_loss[e] /= len(valid_dataloader)\n","    valid_F1[e] /= len(valid_dataloader)\n","    \n","    scheduler.step(valid_loss[e])    \n","    # 학습/검증 결과 산출 표시(loss)\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","        e, train_loss[e], valid_loss[e]))\n","    \n","    # 학습/검증 결과 산출 표시(정확도도)\n","    print('Epoch: {} \\tTraining accuracy: {:.6f} \\tValidation accuracy: {:.6f}'.format(\n","        e, train_F1[e], valid_F1[e]))\n","    \n","    # 검증 손실율이 줄어들었을 때 모델 저장장\n","    if valid_loss[e] <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss[e]))\n","        torch.save(model.state_dict(), 'swin_tiny_patch4_window7_224.pt')\n","        valid_loss_min = valid_loss[e]"],"metadata":{"id":"omALsJxARLyB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 모델 로드"],"metadata":{"id":"4PSoBbmARV5D"}},{"cell_type":"code","source":["model.load_state_dict(torch.load('swin_tiny_patch4_window7_224.pt'))"],"metadata":{"id":"7x2bltoFRTMD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TestDataset(torch.utils.data.Dataset):\n","    def __init__(self, x_dir,transform=None):\n","        super().__init__()\n","        self.transforms = transform\n","        self.x_img = x_dir \n","\n","    def __len__(self):\n","        return len(self.x_img)\n","\n","    def __getitem__(self, idx):\n","        x_img = self.x_img[idx]\n","\n","        x_img = cv2.imread(x_img)\n","        x_img = cv2.cvtColor(x_img, cv2.COLOR_BGR2RGB)\n","\n","        if self.transforms:\n","            augmented = self.transforms(image=x_img)\n","            x_img = augmented['image']\n","\n","        return x_img"],"metadata":{"id":"NcyRQdd1RXzh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["means=(0.5,)\n","stds=(0.5,)\n","testtransform = albumentations.Compose([\n","      albumentations.Resize(224, 224),\n","      albumentations.augmentations.transforms.Normalize(mean=means, std=stds, p=1.0),\n","      albumentations.pytorch.transforms.ToTensorV2(p=1.0)\n","      ])"],"metadata":{"id":"nePi9u5BRZ1h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size=64\n","test_dataset = TestDataset(test,testtransform)\n","test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"],"metadata":{"id":"qumbqnV6Rbcx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 추론(TTA)"],"metadata":{"id":"gieRRF3kReKh"}},{"cell_type":"code","source":[],"metadata":{"id":"lIoT2CwfRf65"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 모델 앙상블"],"metadata":{"id":"dWX0M9yQRk5w"}},{"cell_type":"code","source":[" def transform_inputsize(inputsize):\n","  return albumentations.Compose([\n","      albumentations.Resize(inputsize, inputsize),\n","      albumentations.augmentations.transforms.Normalize(mean=(0.5,), std=(0.5,), p=1.0),\n","      albumentations.pytorch.transforms.ToTensorV2(p=1.0)\n","      ])"],"metadata":{"id":"7XCraIMtRmg8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size=32\n","test_dataset224 = TestDataset(test,transform_inputsize(224))\n","test_loader224 = torch.utils.data.DataLoader(test_dataset224, shuffle=False, batch_size=batch_size)\n","test_dataset300 = TestDataset(test,transform_inputsize(300))\n","test_loader300 = torch.utils.data.DataLoader(test_dataset300, shuffle=False, batch_size=batch_size)"],"metadata":{"id":"8Auy3w4yRokC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["모델 로드"],"metadata":{"id":"Bbz9GOcvRrzg"}},{"cell_type":"code","source":["swin_tiny_patch4_window7_224 = timm.create_model('swin_tiny_patch4_window7_224',pretrained=True,num_classes=88,in_chans=3)\n","swin_tiny_patch4_window7_224.load_state_dict(torch.load('swin_tiny_patch4_window7_224.pt'))\n","mixnet_s=timm.create_model('mixnet_s',pretrained=True,num_classes=88,in_chans=3)\n","mixnet_s.load_state_dict(torch.load('mixnet_s.pt'))"],"metadata":{"id":"W35SKrv3Rqzi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["efficientnetB2 = EfficientNet.from_pretrained('efficientnet-b2', num_classes=88)\n","efficientnetB2.load_state_dict(torch.load('efficientnet-b2.pt'))\n","efficientnetB0 = EfficientNet.from_pretrained('efficientnet-b0', num_classes=88)\n","efficientnetB0.load_state_dict(torch.load('efficientnet-b0.pt'))"],"metadata":{"id":"Q1zoWPSiRvHh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tta_swin_tiny_patch4_window7_224 = tta.ClassificationTTAWrapper(swin_tiny_patch4_window7_224, tta_transforms)\n","tta_mixnet_s = tta.ClassificationTTAWrapper(mixnet_s, tta_transforms) \n","tta_efficientnetB2 = tta.ClassificationTTAWrapper(efficientnetB2, tta_transforms) \n","tta_efficientnetB0 = tta.ClassificationTTAWrapper(efficientnetB0, tta_transforms) "],"metadata":{"id":"vpIZZ7lURxp4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["추론"],"metadata":{"id":"0K2vrM5FR1BA"}},{"cell_type":"code","source":["tta_swin_tiny_patch4_window7_224.to(device)\n","tta_swin_tiny_patch4_window7_224.eval()\n","tta_mixnet_s.to(device)\n","tta_mixnet_s.eval()\n","tta_efficientnetB2.to(device)\n","tta_efficientnetB2.eval()\n","tta_efficientnetB0.to(device)\n","tta_efficientnetB0.eval()\n","f_pred = []\n","\n","with torch.no_grad():\n","    for img224,img300 in iter(zip(test_loader224,test_loader300)):\n","\n","        img224 = img224.to(device)\n","        img300 = img300.to(device)\n","\n","        pred1 = tta_swin_tiny_patch4_window7_224(img224)\n","        pred2 = tta_efficientnetB2(img300)\n","        pred3 = tta_efficientnetB0(img300)\n","        pred4 = tta_mixnet_s(img300)\n","\n","        pred = (pred1+pred2+pred3+pred4)/4\n","        \n","        f_pred.extend(pred.argmax(1).detach().cpu().numpy().tolist())"],"metadata":{"id":"nWtGT1JvR0AZ"},"execution_count":null,"outputs":[]}]}