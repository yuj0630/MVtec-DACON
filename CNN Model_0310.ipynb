{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNyGFGBFSQ8QTctpKiVjjl9"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"tNfB_uEO7ndl"},"outputs":[],"source":["!pip install timm"]},{"cell_type":"code","source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","from glob import glob\n","import pandas as pd\n","import numpy as np \n","from tqdm import tqdm\n","import cv2\n","\n","import os\n","import timm\n","import random\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","from sklearn.metrics import f1_score, accuracy_score\n","import time\n","\n","\n","\n","device = torch.device('cuda')"],"metadata":{"id":"ckFuDVYg70AU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SEED = 42\n","\n","def reset_seeds(seed):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)    # 파이썬 환경변수 시드 고정\n","    np.random.seed(seed)\n","    torch.manual_seed(seed) # cpu 연산 무작위 고정\n","    torch.cuda.manual_seed(seed) # gpu 연산 무작위 고정\n","    torch.backends.cudnn.deterministic = True  # cuda 라이브러리에서 Deterministic(결정론적)으로 예측하기 (예측에 대한 불확실성 제거 )\n","\n","reset_seeds(SEED)"],"metadata":{"id":"FAgKpn10DA1B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/DL_Project\")"],"metadata":{"id":"mMreQTQI717Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_png = sorted(glob('train/*.png'))\n","test_png = sorted(glob('test/*.png'))"],"metadata":{"id":"UjoBxVN073Tg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_png), len(test_png)"],"metadata":{"id":"XIkdKKSh75O7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_y = pd.read_csv(\"/content/drive/MyDrive/DL_Project/train_df.csv\")\n","\n","train_labels = train_y[\"label\"]\n","\n","label_unique = sorted(np.unique(train_labels))\n","label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n","\n","train_labels = [label_unique[k] for k in train_labels]"],"metadata":{"id":"txDi49Z077Bp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def img_load(path): # 회색 변경, 리사이즈즈\n","  img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n","  img = cv2.imread(path[:,:,::-1])\n","  img = cv2.resize(img, (256, 256))"],"metadata":{"id":"ZkzHcNwz78Px"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_imgs = [img_load(m) for m in tqdm(train_256_png)] # train_imgs 다운로드\n","test_imgs = [img_load(n) for n in tqdm(test_png)] # test_imgs 다운로드"],"metadata":{"id":"AdZNGBrz8VbO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Custom_dataset(Dataset): # 여긴 같음음\n","   def __init__(self, img_paths, labels, mode='train'): \n","        self.img_paths = img_paths\n","        self.labels = labels\n","        self.mode=mode\n","   def __len__(self):\n","        return len(self.img_paths)\n","   def __getitem__(self, idx):\n","       img = self.img_paths[idx]\n","       if self.mode=='train':\n","           augmentation = random.randint(0,2)\n","           if augmentation==1:\n","               img = img[::-1].copy()\n","           elif augmentation==2:\n","               img = img[:,::-1].copy()\n","       img = transforms.ToTensor()(img)\n","       if self.mode=='test':\n","           pass\n","        \n","       label = self.labels[idx]\n","       return img, label\n","\n","class Network(nn.Module): # CNN, baseline/non linear module (ReLU 차이)\n","    def __init__(self, input_shape: int, hidden_units: int, output_shape: int):\n","        super(Network, self).__init__()\n","        self.layer_stack = nn.Sequential(\n","            nn.Flatten(),\n","            nn.Linear(in_features=input_shape, out_features=hidden_units),\n","            nn.ReLU(),\n","            nn.Linear(in_features=hidden_units, out_features=output_shape),\n","            nn.ReLU()\n","        )\n","    def forward(self, x: torch.Tensor):\n","        return return self.layer_stack(x)        "],"metadata":{"id":"WoEFxVQ28ZX5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.manual_seed(SEED)\n","\n","baseline_model = BaselineModel(input_shape=784,\n","                               hidden_units=10,\n","                               output_shape=len(class_names)\n","                               )\n","baseline_model.to(device)"],"metadata":{"id":"uLjeODQ2VChu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 여기는 score를 계산하는 것으로 보입니다.\n","def score_function(real, pred):\n","  score = f1_score(real, pred, average=\"macro\")\n","  return score"],"metadata":{"id":"CRlvWVpbDMAH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset : 전체 dataset 구성 dataloader : mini batch 만드는 역할할\n","batch_size = 32 # batch_size : 사진들을 몇 개 묶음으로 할 거냐\n","epochs = 50 # 학습 시도 횟수\n","\n","# 데이터 셋과 데이터 로더 부분\n","\n","# Train\n","train_dataset = Custom_dataset(np.array(train_imgs), np.array(train_labels), mode='train') # train 데이터셋 학습 모델\n","train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n","\n","# Test\n","test_dataset = Custom_dataset(np.array(test_imgs), np.array([\"tmp\"]*len(test_imgs)), mode='test') # test 데이터셋 학습 모델\n","test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"],"metadata":{"id":"WQmxKrnXDR3G"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def score_function(real, pred):\n","    score = f1_score(real, pred, average=\"macro\")\n","    return score # 모델 스코어\n","\n","model = Network().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3) # learning rate 설정정\n","criterion = nn.CrossEntropyLoss() # 기본 nCELoss\n","scaler = torch.cuda.amp.GradScaler() #cuda 제공 기본      \n","\n","best=0\n","for epoch in range(epochs): # 학습 기본 설정 setting\n","    start=time.time()\n","    train_loss = 0\n","    train_pred=[]\n","    train_y=[]\n","    model.train()\n","    for batch in (train_loader):\n","        optimizer.zero_grad()\n","        x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n","        y = torch.tensor(batch[1], dtype=torch.long, device=device)\n","        with torch.cuda.amp.autocast():\n","            pred = model(x)\n","        loss = criterion(pred, y)\n","        \n","############################## 여기가 학습 ##############################\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        \n","        train_loss += loss.item()/len(train_loader)\n","        train_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n","        train_y += y.detach().cpu().numpy().tolist()\n","        \n","    \n","    train_f1 = score_function(train_y, train_pred)\n","\n","    TIME = time.time() - start\n","    print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s/{TIME*(epochs-epoch-1):.0f}s') # 시간 확인 안내내\n","    print(f'TRAIN    loss : {train_loss:.5f}    f1 : {train_f1:.5f}')"],"metadata":{"id":"Id_elegqDTuA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.eval()\n","f_pred = [] \n","\n","with torch.no_grad():\n","    for batch in (test_loader):\n","        x = torch.tensor(batch[0], dtype = torch.float32, device = device)\n","        with torch.cuda.amp.autocast():\n","            pred = model(x)\n","        f_pred.extend(pred.argmax(1).detach().cpu().numpy().tolist())"],"metadata":{"id":"vHhK3X_NFUt1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Baseline model"],"metadata":{"id":"H2kN_7SsYuOx"}},{"cell_type":"code","source":["# Import tqdm for progress bar\n","from tqdm.auto import tqdm\n","\n","# Set the seed and start the timer\n","torch.manual_seed(42)\n","train_time_start_on_cpu = timer()\n","\n","# Set the number of epochs (we'll keep this small for faster training times)\n","epochs = 3\n","\n","# Create training and testing loop\n","for epoch in tqdm(range(epochs)):\n","    print(f\"Epoch: {epoch}\\n-------\")\n","    ### Training\n","    train_loss = 0\n","    # Add a loop to loop through training batches\n","    for batch, (X, y) in enumerate(train_dataloader):\n","        baseline_model.train() \n","        # 1. Forward pass\n","        y_pred = baseline_model(X)\n","\n","        # 2. Calculate loss (per batch)\n","        loss = loss_fn(y_pred, y)\n","        train_loss += loss # accumulatively add up the loss per epoch \n","\n","        # 3. Optimizer zero grad\n","        optimizer.zero_grad()\n","\n","        # 4. Loss backward\n","        loss.backward()\n","\n","        # 5. Optimizer step\n","        optimizer.step()\n","\n","        # Print out how many samples have been seen\n","        if batch % 400 == 0:\n","            print(f\"Looked at {batch * len(X)}/{len(train_dataloader.dataset)} samples\")\n","\n","    # Divide total train loss by length of train dataloader (average loss per batch per epoch)\n","    train_loss /= len(train_dataloader)\n","    \n","    ### Testing\n","    # Setup variables for accumulatively adding up loss and accuracy \n","    test_loss, test_acc = 0, 0 \n","    baseline_model.eval()\n","    with torch.inference_mode():\n","        for X, y in test_dataloader:\n","            # 1. Forward pass\n","            test_pred = baseline_model(X)\n","\n","            # 2. Calculate loss (accumatively)\n","            test_loss += loss_fn(test_pred, y) # accumulatively add up the loss per epoch\n","\n","            # 3. Calculate accuracy (preds need to be same as y_true)\n","            test_acc += accuracy_fn(y_true=y, y_pred=test_pred.argmax(dim=1))\n","        \n","        # Calculations on test metrics need to happen inside torch.inference_mode()\n","        # Divide total test loss by length of test dataloader (per batch)\n","        test_loss /= len(test_dataloader)\n","\n","        # Divide total accuracy by length of test dataloader (per batch)\n","        test_acc /= len(test_dataloader)\n","\n","    ## Print out what's happening\n","    print(f\"\\nTrain loss: {train_loss:.5f} | Test loss: {test_loss:.5f}, Test acc: {test_acc:.2f}%\\n\")\n","\n","# Calculate training time      \n","train_time_end_on_cpu = timer()\n","total_train_time_baseline_model = print_train_time(start=train_time_start_on_cpu, \n","                                                    end=train_time_end_on_cpu,\n","                                                    device=str(next(baseline_model.parameters()).device))"],"metadata":{"id":"hgOo2Q_XYtsp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 데이터를 나눈 뒤 전처리를 해야 함\n","- 전처리한 데이터를 합친 뒤 기본 모델을 사용하여 스코어 계산"],"metadata":{"id":"ZtCtNqkAY-hv"}},{"cell_type":"code","source":[],"metadata":{"id":"06BddhDEZNbW"},"execution_count":null,"outputs":[]}]}