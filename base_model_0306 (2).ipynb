{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1fOrPhevoOPUoHTIYOqwVddjhhGRjP_v_","timestamp":1678088052519}],"mount_file_id":"1mAoc2TVAnOjdaViMwZ-mm27BciznEKKo","authorship_tag":"ABX9TyPejh82kHmrevUeIE7tIu+y"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# 라이브러리 불러오기"],"metadata":{"id":"WgOpgLe0h7pg"}},{"cell_type":"code","source":["!pip install timm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YHF3bJWr1RpZ","executionInfo":{"status":"ok","timestamp":1679011869679,"user_tz":-540,"elapsed":8044,"user":{"displayName":"정영운","userId":"08661002276326861946"}},"outputId":"9267174f-7391-4126-87fa-dbd08a1c093c"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm\n","  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub\n","  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm) (0.14.1+cu116)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from timm) (6.0)\n","Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.9/dist-packages (from timm) (1.13.1+cu116)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (4.5.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (23.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (2.25.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (3.9.1)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (4.65.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->timm) (8.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->timm) (1.22.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (4.0.0)\n","Installing collected packages: huggingface-hub, timm\n","Successfully installed huggingface-hub-0.13.2 timm-0.6.12\n"]}]},{"cell_type":"code","source":["!pip install -U albumentations"],"metadata":{"id":"C5vs2N8Gxzy-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679011892996,"user_tz":-540,"elapsed":9484,"user":{"displayName":"정영운","userId":"08661002276326861946"}},"outputId":"2b9144db-67fb-4392-ff3a-06c3e60ea183"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: albumentations in /usr/local/lib/python3.9/dist-packages (1.2.1)\n","Collecting albumentations\n","  Downloading albumentations-1.3.0-py3-none-any.whl (123 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 KB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from albumentations) (1.10.1)\n","Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.9/dist-packages (from albumentations) (1.22.4)\n","Requirement already satisfied: qudida>=0.0.4 in /usr/local/lib/python3.9/dist-packages (from albumentations) (0.0.4)\n","Requirement already satisfied: opencv-python-headless>=4.1.1 in /usr/local/lib/python3.9/dist-packages (from albumentations) (4.7.0.72)\n","Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.9/dist-packages (from albumentations) (0.19.3)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.9/dist-packages (from albumentations) (6.0)\n","Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.9/dist-packages (from qudida>=0.0.4->albumentations) (1.2.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from qudida>=0.0.4->albumentations) (4.5.0)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations) (3.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations) (2023.2.28)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations) (23.0)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations) (2.9.0)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations) (8.4.0)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-image>=0.16.1->albumentations) (1.4.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.9/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.1)\n","Installing collected packages: albumentations\n","  Attempting uninstall: albumentations\n","    Found existing installation: albumentations 1.2.1\n","    Uninstalling albumentations-1.2.1:\n","      Successfully uninstalled albumentations-1.2.1\n","Successfully installed albumentations-1.3.0\n"]}]},{"cell_type":"code","source":["!pip install ttach"],"metadata":{"id":"-51AYOYgx0pR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679011900344,"user_tz":-540,"elapsed":4926,"user":{"displayName":"정영운","userId":"08661002276326861946"}},"outputId":"97075162-c1bc-43fe-b8d3-39f389b7d982"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting ttach\n","  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n","Installing collected packages: ttach\n","Successfully installed ttach-0.0.3\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"id":"QBMjGyLGgyvP","executionInfo":{"status":"ok","timestamp":1679011911742,"user_tz":-540,"elapsed":7144,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","from glob import glob\n","import pandas as pd\n","import numpy as np \n","from tqdm import tqdm\n","import cv2\n","\n","import os\n","import timm\n","import random\n","\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import torch.nn as nn\n","import torchvision.transforms as transforms\n","from sklearn.metrics import f1_score, accuracy_score\n","import time\n","\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","import ttach as tta\n","\n","device = torch.device('cuda')"]},{"cell_type":"markdown","source":["# 원 데이터를 불러옵니다..."],"metadata":{"id":"A2nvIh_43c44"}},{"cell_type":"code","source":["os.chdir(\"/content/drive/MyDrive/DL_Project\")"],"metadata":{"id":"5V481GB6xHxn","executionInfo":{"status":"ok","timestamp":1679011922509,"user_tz":-540,"elapsed":671,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["train_png = sorted(glob('train/*.png'))\n","test_png = sorted(glob('test/*.png'))"],"metadata":{"id":"snNQR0Y8g2OS","executionInfo":{"status":"ok","timestamp":1679011990469,"user_tz":-540,"elapsed":66775,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["len(train_png), len(test_png)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hy9ykBBB1g-A","executionInfo":{"status":"ok","timestamp":1679012171672,"user_tz":-540,"elapsed":3,"user":{"displayName":"정영운","userId":"08661002276326861946"}},"outputId":"6863107d-ca73-44d7-bd93-c0f23dc51d39"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(7225, 2154)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["train_y = pd.read_csv(\"/content/drive/MyDrive/DL_Project/train_df.csv\")\n","\n","train_labels = train_y[\"label\"]\n","\n","label_unique = sorted(np.unique(train_labels))\n","label_unique = {key:value for key,value in zip(label_unique, range(len(label_unique)))}\n","\n","train_labels = [label_unique[k] for k in train_labels]"],"metadata":{"id":"BWEKla_shQDy","executionInfo":{"status":"ok","timestamp":1679012172579,"user_tz":-540,"elapsed":909,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# 가중치 정규화(가 뭘까)?"],"metadata":{"id":"dx3TKA5w1374"}},{"cell_type":"code","source":["def img_load(path):\n","    img = cv2.imread(path)[:,:,::-1] # 역으로 읽기기\n","    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE) # 흑백 변환\n","    img = cv2.resize(img, (384, 384))\n","    return img"],"metadata":{"id":"ht4fhxgKhSGW","executionInfo":{"status":"ok","timestamp":1679012180896,"user_tz":-540,"elapsed":4,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["train_imgs = [img_load(m) for m in tqdm(train_png)] # train_imgs 다운로드\n","test_imgs = [img_load(n) for n in tqdm(test_png)] # test_imgs 다운로드드"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ECSAs80mhj_7","outputId":"8b4f6dec-3d97-42ad-9933-6edfb8895dec","executionInfo":{"status":"ok","timestamp":1678807382171,"user_tz":-540,"elapsed":3486392,"user":{"displayName":"정영운","userId":"08661002276326861946"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 7225/7225 [11:19<00:00, 10.63it/s]\n","100%|██████████| 2154/2154 [46:46<00:00,  1.30s/it]\n"]}]},{"cell_type":"markdown","source":["# 들어가기 전 말할 것들\n","- 이번 optimizer는 통상적으로 adam을 쓸 거예요\n","- 제일 안정적으로 결과가 잘 나오는 것이 adam이라서 그렇습니다\n","- 그리고 efficientnet 모델 종류는 여러가지를 써 봅시다"],"metadata":{"id":"P5uMKgUf4GBe"}},{"cell_type":"code","source":["train_transform = transforms.Compose([\n","    transforms.RandomCrop(356),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.RandomVerticalFilp(),\n","    transforms.ToTensor(),\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","])\n","\n","\n","test_transform = transforms.Compose([\n","    transforms.ToTensor(),  # 이 과정에서 [0, 255]의 범위를 갖는 값들을 [0.0, 1.0]으로 정규화, torch.FloatTensor로 변환\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])   #  정규화(normalization)\n","    \n","])"],"metadata":{"id":"RbJX_R-bmBF8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Custom_dataset(Dataset):\n","    def __init__(self, img_paths, labels, mode='train'):\n","        self.img_paths = img_paths\n","        self.labels = labels\n","        self.mode=mode\n","    def __len__(self):\n","        return len(self.img_paths)\n","    def __getitem__(self, idx):\n","        img = self.img_paths[idx]\n","        if self.mode=='train':\n","            img = train_transform(image=img)\n","        if self.mode=='test':\n","            img = test_transform(image=img)\n","        \n","        label = self.labels[idx]\n","        return img, label\n","    \n","class Network(nn.Module):\n","    def __init__(self):\n","        super(Network, self).__init__()\n","        self.model = timm.create_model('efficientnet_b4', pretrained=True, num_classes=88)\n","        \n","    def forward(self, x):\n","        x = self.model(x)\n","        return x"],"metadata":{"id":"pYAAzX9Khqrw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 여기는 score를 계산하는 것으로 보입니다.\n","def score_function(real, pred):\n","  score = f1_score(real, pred, average=\"macro\")\n","  return score"],"metadata":{"id":"1tgTXLDJ2c6p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 데이터 셋과 데이터 로더\n","- "],"metadata":{"id":"mIzVnrbS3kwg"}},{"cell_type":"code","source":["# dataset : 전체 dataset 구성 dataloader : mini batch 만드는 역할할\n","batch_size = 32 # batch_size : 사진들을 몇 개 묶음으로 할 거냐\n","epochs = 70 # 학습 시도 횟수\n","\n","# 데이터 셋과 데이터 로더 부분\n","\n","# Train\n","train_dataset = Custom_dataset(np.array(train_imgs), np.array(train_labels), mode='train') # train 데이터셋 학습 모델\n","train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size)\n","\n","# Test\n","test_dataset = Custom_dataset(np.array(test_imgs), np.array([\"tmp\"]*len(test_imgs)), mode='test') # test 데이터셋 학습 모델\n","test_loader = DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"],"metadata":{"id":"wL6iwVRGhrQ3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# training"],"metadata":{"id":"fkCuA_mI264n"}},{"cell_type":"markdown","source":["- 여기가 코딩함에 따라 결과가 바뀌는 과정 == yolov5 train.py \n","- 훈련의 예시에는 여러 개가 있고 밑에 예시를 가져올게요"],"metadata":{"id":"uE6se6n33L_X"}},{"cell_type":"code","source":["def score_function(real, pred):\n","    score = f1_score(real, pred, average=\"macro\")\n","    return score # 모델 스코어\n","\n","model = Network().to(device)\n","\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","scaler = torch.cuda.amp.GradScaler() \n","\n","\n","\n","best=0\n","for epoch in range(epochs): # 학습 기본 설정 setting\n","    start=time.time()\n","    train_loss = 0\n","    train_pred=[]\n","    train_y=[]\n","    model.train()\n","    for batch in (train_loader):\n","        optimizer.zero_grad()\n","        x = torch.tensor(batch[0], dtype=torch.float32, device=device)\n","        y = torch.tensor(batch[1], dtype=torch.long, device=device)\n","        with torch.cuda.amp.autocast():\n","            pred = model(x)\n","        loss = criterion(pred, y)\n","\n","        if len(y) != len(pred):\n","            continue\n","\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        \n","        train_loss += loss.item()/len(train_loader)\n","        train_pred += pred.argmax(1).detach().cpu().numpy().tolist()\n","        train_y += y.detach().cpu().numpy().tolist()\n","        \n","    \n","    train_f1 = score_function(train_y, train_pred)\n","\n","    TIME = time.time() - start\n","    print(f'epoch : {epoch+1}/{epochs}    time : {TIME:.0f}s/{TIME*(epochs-epoch-1):.0f}s') # 시간 확인 안내내\n","    print(f'TRAIN    loss : {train_loss:.5f}    f1 : {train_f1:.5f}')\n","    if train_f1 > 0.9980:\n","      model_path = \"/content/drive/MyDrive/DL_Project/model\"\n","      torch.save(model.state_dict(), f\"{train_f1}.pt\")"],"metadata":{"id":"kzUyfNKshwXm","colab":{"base_uri":"https://localhost:8080/","height":396},"executionInfo":{"status":"error","timestamp":1678809866300,"user_tz":-540,"elapsed":799,"user":{"displayName":"정영운","userId":"08661002276326861946"}},"outputId":"7a4da2d6-5a7e-4081-f657-9a7991b6babf"},"execution_count":null,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-24a34dc010da>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mtrain_y\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-b84d8d492101>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index 5973 is out of bounds for axis 0 with size 4277"]}]},{"cell_type":"markdown","source":["# 모델 학습"],"metadata":{"id":"mRln-p0xh2rn"}},{"cell_type":"markdown","source":["# 모델 추론\n","- 이건 말 그대로 모델의 학습이 끝난 직후, 어떤 방식으로 모델의 정확도를 높일 수 있을 지를 생각하는 코드입니다.\n","- 여기도 바꿔줘야 되요!"],"metadata":{"id":"8n2uJWRbjTzX"}},{"cell_type":"code","source":["model.eval()\n","f_pred = [] \n","\n","with torch.no_grad():\n","    for batch in (test_loader):\n","        x = torch.tensor(batch[0], dtype = torch.float32, device = device)\n","        with torch.cuda.amp.autocast():\n","            pred = model(x)\n","        f_pred.extend(pred.argmax(1).detach().cpu().numpy().tolist())"],"metadata":{"id":"P392eEfLjPGA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label_decoder = {val:key for key, val in label_unique.items()} \n","\n","f_result = [label_decoder[result] for result in f_pred]"],"metadata":{"id":"TOVkXR0Zjb_O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 결과창"],"metadata":{"id":"LRRRVihU3u5m"}},{"cell_type":"code","source":["submission = pd.read_csv(\"open/sample_submission.csv\") # 데이터 제출출\n","\n","submission[\"label\"] = f_result\n","\n","submission"],"metadata":{"id":"lRMRKSu9jck2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 여기까지가 dacon에서 제공한 base_model입니다\n","- 나머지는 저희가 발전시키면 되겠죠?"],"metadata":{"id":"K5z8a8OQ2g4x"}},{"cell_type":"markdown","source":["# 모델 정확도 증가 방법"],"metadata":{"id":"xV7wfg3Gzbqr"}},{"cell_type":"markdown","source":["## 데이터 증강(data argumentation)\n","\n","- 여기서 TTA란 기법이 나오며, test-time argumentation이라고 합니다. tta는 모델 학습 중에 데이터 증강을 도입하는 것이라고 보면 될 것 같아요\n","- 데이터 증강이란 거는 쉽게 말해서 뒤집고 돌리고 한다고 보시면 됩니다."],"metadata":{"id":"Z-e8dNkaOBHr"}},{"cell_type":"code","source":["!pip install albumentation==0.4.6"],"metadata":{"id":"ZNfv4B4ROSPh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 하는 방법(예시)\n","import albumentations\n","import albumentations.pytorch\n","\n","aug = albumentations.Compose([\n","      albumentations.Resize(224, 224),\n","      albumentations.HorizontalFlip(), #수평 뒤집기\n","      albumentations.VerticalFlip(), #수직 뒤집기\n","      albumentations.OneOf([\n","                          albumentations.Rotate(), # 돌리기기\n","                          albumentations.ShiftScaleRotate()\n"," \n","      ], p=1),\n","      albumentations.augmentations.transforms.Normalize(mean=(0.5,), std=(0.5,), p=1.0), #정규화화\n","      albumentations.pytorch.transforms.ToTensorV2(p=1.0)\n","      ])\n","aug2 = albumentations.Compose([\n","      albumentations.Resize(224, 224),\n","      albumentations.Rotate(), # 돌리기기\n","      albumentations.augmentations.transforms.Normalize(mean=(0.5,), std=(0.5,), p=1.0),\n","      albumentations.pytorch.transforms.ToTensorV2(p=1.0)\n","      ])"],"metadata":{"id":"IgYuMjp3OAao"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 이미지 기법에 대한 더 자세한 설명은 강사님이 주신 자료 중 7-1. image-processing의 동명의 파일 \n","- image-processing를 보면 더 자세히 나와 있어요."],"metadata":{"id":"E9CZk02s451X"}},{"cell_type":"markdown","source":["## 앙상블 기법\n","- 여러분이 아시는 그 앙상블로, 파라미터나 epoch, batch, resize 등 다양한 기법으로 만들어진 모델 중 \n","- 괜찮은 것들을 찾아 앙상블하는 것입니다."],"metadata":{"id":"d9nvDEF-3cP-"}},{"cell_type":"code","source":[],"metadata":{"id":"5tMs7UuT213g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 학습(다른 예시)\n","- 1번째 거는 다른 팀들이 학습 코드를 짠 것들이에요\n"],"metadata":{"id":"xdmLcGtgO2tO"}},{"cell_type":"code","source":["# 시도 횟수\n","n_epochs = 100\n","valid_loss_min = np.inf # 100보다 작을 시 멈춤\n","\n","# keep track of training and validation loss\n","train_loss = torch.zeros(n_epochs)\n","valid_loss = torch.zeros(n_epochs)\n","\n","train_F1 = torch.zeros(n_epochs)\n","valid_F1 = torch.zeros(n_epochs)\n","model.to(device)\n","\n","for e in range(0, n_epochs):\n","\n","   \n","    ###################\n","    # 모델 학습       #\n","    ###################\n","    model.train()\n","    for data, labels in tqdm(train_dataloader):\n","        # move tensors to GPU if CUDA is available\n","        data, labels = data.to(device), labels.to(device)\n","        # 순전파 :compute predicted outputs by passing inputs to the model\n","        logits = model(data)\n","        # 배치의 손실율 계산\n","        loss = criterion(logits, labels)\n","\n","        optimizer.zero_grad()\n","        # 역전파 : compute gradient of the loss with respect to model parameters\n","        loss.backward()\n","        # 싱글 옵티미제이션(adam)\n","        optimizer.step()\n","        # 학습 손실율 업데이트\n","        train_loss[e] += loss.item()\n","        # 학습 스코어 계산산\n","        logits=logits.argmax(1).detach().cpu().numpy().tolist()\n","        labels=labels.detach().cpu().numpy().tolist()\n","\n","        train_F1[e] += score_function(labels,logits)\n","\n","    train_loss[e] /= len(train_dataloader)\n","    train_F1[e] /= len(train_dataloader)\n","        \n","        \n","    ######################    \n","    # 검증 모델          #\n","    ######################\n","    with torch.no_grad(): \n","        model.eval()\n","        for data, labels in tqdm(valid_dataloader):\n","            # move tensors to GPU if CUDA is available\n","            data, labels = data.to(device), labels.to(device)\n","            # 순전파: compute predicted outputs by passing inputs to the model\n","            logits = model(data)\n","            # 배치 손실율 계산\n","            loss = criterion(logits, labels)\n","            # 평균 검증 손실율 \n","            valid_loss[e] += loss.item()\n","            # update training score\n","            logits=logits.argmax(1).detach().cpu().numpy().tolist()\n","            labels=labels.detach().cpu().numpy().tolist()\n","            valid_F1[e] += score_function(labels,logits)\n","            \n","    \n","    # 평균 손실율 계산산\n","    valid_loss[e] /= len(valid_dataloader)\n","    valid_F1[e] /= len(valid_dataloader)\n","    \n","    scheduler.step(valid_loss[e])    \n","    # 학습/검증 결과 산출 표시(loss)\n","    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n","        e, train_loss[e], valid_loss[e]))\n","    \n","    # 학습/검증 결과 산출 표시(정확도도)\n","    print('Epoch: {} \\tTraining accuracy: {:.6f} \\tValidation accuracy: {:.6f}'.format(\n","        e, train_F1[e], valid_F1[e]))\n","    \n","    # 검증 손실율이 줄어들었을 때 모델 저장장\n","    if valid_loss[e] <= valid_loss_min:\n","        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n","        valid_loss_min,\n","        valid_loss[e]))\n","        torch.save(model.state_dict(), 'swin_tiny_patch4_window7_224.pt')\n","        valid_loss_min = valid_loss[e]"],"metadata":{"id":"w9m1iBf8O2Gg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 여기서 순전파 (forward)랑 역전파(backward)는 강사님이 주신 자료를 보시면 이해가 빠르실 겁니다 "],"metadata":{"id":"PeeSoRI34i3n"}},{"cell_type":"code","source":["model.load_state_dict(torch.load('swin_tiny_patch4_window7_224.pt'))"],"metadata":{"id":"oQRgnLRcQa1N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class TestDataset(torch.utils.data.Dataset):\n","    def __init__(self, x_dir,transform=None):\n","        super().__init__()\n","        self.transforms = transform\n","        self.x_img = x_dir \n","\n","    def __len__(self):\n","        return len(self.x_img)\n","\n","    def __getitem__(self, idx):\n","        x_img = self.x_img[idx]\n","\n","        x_img = cv2.imread(x_img)\n","        x_img = cv2.cvtColor(x_img, cv2.COLOR_BGR2RGB)\n","\n","        if self.transforms:\n","            augmented = self.transforms(image=x_img)\n","            x_img = augmented['image']\n","\n","        return x_img"],"metadata":{"id":"Mgmw4sRaQdcN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["means=(0.5,)\n","stds=(0.5,)\n","testtransform = albumentations.Compose([\n","      albumentations.Resize(224, 224),\n","      albumentations.augmentations.transforms.Normalize(mean=means, std=stds, p=1.0),\n","      albumentations.pytorch.transforms.ToTensorV2(p=1.0)\n","      ])"],"metadata":{"id":"fDkUNS5aQfv9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size=64\n","test_dataset = TestDataset(test,testtransform)\n","test_loader = torch.utils.data.DataLoader(test_dataset, shuffle=False, batch_size=batch_size)"],"metadata":{"id":"2q2j907mQiy8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["- 이 위의 것들은 모델들을 로드하고 다시 결과를 산출하는 과정이라 보시면 될 듯 해요"],"metadata":{"id":"SjGcllP15bXe"}},{"cell_type":"markdown","source":["# 결론(제일 중요)\n","- 우리가 해야 될 것은 뭘까요?\n","    - 1. 이미지 전처리(기법 활용)를 모델에 맞춰서 잘 하는 것\n","    - 2. effiecientb0~b7의 모델 중 좋은 모델을 찾아서\n","    - 3. training & validation 코드를 잘 짜고 (전체적인 과정 이해하면 더 좋아요!)\n","    - 4. 결과값이 나오면 어떻게 parameter를 만질지 고민, image-argumentation, ensemble, 5-fold 적극 활용!\n","    - 5. 최대한 다양한 모델을 만들어 보고 최적의 결과 찾기!"],"metadata":{"id":"MLAobuz6QlYk"}},{"cell_type":"markdown","source":["# 여담 \n","\n","- 밑에 것들은 기본 인자값들을 설정해주는 프로그램인데 우리가 이걸 구현할 지는 모르겠어요"],"metadata":{"id":"pMWpvnix1ndn"}},{"cell_type":"code","source":["config = { # 전역 파라미터 변수 설정정\n","    # Model parameters\n","    'model': 'efficientnet_b0',\n","    'batch_size': 32,\n","    'pretrain': True,\n","    \n","    # Optimizer parameters\n","    'optimizer': 'AdamW',\n","    'lr': 2e-4,\n","    'lr_t': 15,\n","    'lr_scheduler': 'CosineAnnealingWarmUpRestarts',\n","    'gamma': 0.524,\n","    'loss_function': 'CE_with_Lb',\n","    'patience': 10,\n","    'weight_decay': 0.002157,\n","    'label_smoothing': 0.8283,\n","    \n","    # Training parameters\n","    'epochs': 200,\n","    'n_fold': 5,\n","    'num_workers': 16,\n","    'text': \"A\",\n","    'device': '0,1,2,3'\n","    }"],"metadata":{"id":"JomYt9wE7W4e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_args_parser(): # 기본 인자값 부여로 보임임\n","    parser = argparse.ArgumentParser('PyTorch Inference', add_help=False)\n","\n","    # Inference parameters\n","    parser.add_argument('--model_save_name', nargs='+', default='load_model', type=str)\n","    parser.add_argument('--model', default='efficientnet_b7', type=str)\n","    parser.add_argument('--batch_size', default=32, type=int)\n","    parser.add_argument('--pretrain', default=True, type=str2bool)\n","    parser.add_argument('--n_fold', default=5, type=int)\n","    parser.add_argument('--num_workers', default=16, type=int)\n","    parser.add_argument('--device', default='0,1,2,3', type=str)\n","    parser.add_argument('--tta', default=True, type=str2bool)\n","    parser.add_argument('--save_name', default='default', type=str)"],"metadata":{"id":"nTJcI5pW7BLs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["마지막으로 상위권 모델 분들 보시면 정말 다양한 기법이 다 소환되니까 해석이 가능하실 때 보시면 좋아요~"],"metadata":{"id":"9ygh79Z25pbe"}}]}